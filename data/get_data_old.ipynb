{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import os  \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(identity_selection, save_path, file_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    identity_selection.to_csv(save_path + file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_race_info(balanced_data):\n",
    "    race_info = []\n",
    "\n",
    "    for race in balanced_data['race'].unique():\n",
    "        race_data = balanced_data[balanced_data['race'] == race]\n",
    "        unique_ids = race_data['id'].nunique()\n",
    "        num_images = len(race_data)\n",
    "\n",
    "        race_info.append({'Race': race, 'Unique IDs': unique_ids, 'Number of Images': num_images})\n",
    "\n",
    "    race_info_df = pd.DataFrame(race_info)\n",
    "    print(race_info_df)\n",
    "\n",
    "def check_data_leakage(balanced_data, set_of_datasets):\n",
    "    \"\"\"\n",
    "    check_data_leakage(balanced_data, {'african_unbalanced': african_unbalanced, 'asian_unbalanced': asian_unbalanced, 'caucasian_unbalanced': caucasian_unbalanced, 'indian_unbalanced': indian_unbalanced})\n",
    "    or\n",
    "    check_data_leakage(balanced_data, {'linear_prob_dataset': linear_prob_dataset})\n",
    "\n",
    "    \"\"\"\n",
    "    for dataset_name, dataset in set_of_datasets.items():\n",
    "        merged_data = pd.merge(dataset, balanced_data, how='outer', indicator=True)\n",
    "\n",
    "        # Rows that appear in both DataFrames\n",
    "        overlap = merged_data[merged_data['_merge'] == 'both']\n",
    "\n",
    "        # If overlap is empty, there is no overlap between the two DataFrames\n",
    "        if overlap.empty:\n",
    "            print(f\"There is no overlap between {dataset_name} and the new balanced data.\")\n",
    "        else:\n",
    "            print(f\"There is overlap between {dataset_name} and the new balanced data.\")\n",
    "\n",
    "        # Check identity leakage\n",
    "        dataset_id = dataset['id'].tolist()\n",
    "        balanced_data_id = balanced_data['id'].tolist()\n",
    "\n",
    "        print(f\"Number of identity overlap between {dataset_name} and balanced data: \", len(set(dataset_id).intersection(balanced_data_id)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "def get_all_data_csv(data_path):\n",
    "    '''get csv of all data, race, id, image_name'''\n",
    "    coloumns = ['image_name', 'id', 'race']\n",
    "    print(data_path)\n",
    "\n",
    "    df = pd.DataFrame(columns=coloumns)\n",
    "    all_image = []\n",
    "    race_to_label = {'Caucasian': 0, 'Indian': 1, 'Asian': 2, 'African': 3}\n",
    "    label_to_race = {0: 'Caucasian', 1: 'Indian', 2: 'Asian', 3: 'African'}\n",
    "\n",
    "    for race in race_to_label.keys():\n",
    "        race_folder = os.path.join(data_path+'/', race)\n",
    "        print(race_folder)\n",
    "        race_label = race_to_label[race]\n",
    "        for id_folder in os.listdir(race_folder):\n",
    "            if not 'm.' in id_folder:\n",
    "                continue\n",
    "            all_id_image = []\n",
    "\n",
    "            id_path = os.path.join(race_folder, id_folder)\n",
    "            id = id_folder.split(\"/\")[0]\n",
    "\n",
    "            for image in os.listdir(id_path):\n",
    "                all_id_image.append(image)\n",
    "            for image in all_id_image:\n",
    "                all_image.append([image, id, race_label])\n",
    "        \n",
    "    df = pd.DataFrame(all_image, columns=coloumns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_csv = get_all_data_csv(\"./race_per_7000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_per_race(race_df, image_per_id_limit=60, total_img=30000, id_per_race=500):\n",
    "    '''\n",
    "    get a new df with 60 image per id\n",
    "    draw id_per_race uniformly \n",
    "    draw image_per_id uniformly\n",
    "    '''\n",
    "    # Group by ID and count the number of images per ID\n",
    "    data_grouped = race_df.groupby(\"id\").count()\n",
    "    \n",
    "    # Filter IDs with at least 'image_per_id_limit' images\n",
    "    data_grouped = data_grouped[data_grouped[\"image_name\"] >= image_per_id_limit]\n",
    "    \n",
    "    # Get a list of IDs that meet the image count criteria\n",
    "    eligible_ids = data_grouped.index.tolist()\n",
    "    \n",
    "    # Shuffle the list of eligible IDs\n",
    "    random.shuffle(eligible_ids)\n",
    "    \n",
    "    # Take the first 'id_per_race' IDs to ensure uniform distribution\n",
    "    selected_ids = eligible_ids[:id_per_race]\n",
    "    \n",
    "    # Filter the DataFrame to include only rows with selected IDs\n",
    "    filtered_id_df = race_df[race_df[\"id\"].isin(selected_ids)]\n",
    "    \n",
    "    # Randomly sample images from each selected ID to meet the 'total_img' requirement\n",
    "    num_selected_images = 0\n",
    "    selected_rows = []\n",
    "    \n",
    "    for id in selected_ids:\n",
    "        id_df = filtered_id_df[filtered_id_df[\"id\"] == id]\n",
    "        num_images_for_id = min(image_per_id_limit, total_img - num_selected_images)\n",
    "        \n",
    "        # Randomly sample 'num_images_for_id' images for the current ID\n",
    "        sampled_rows = id_df.sample(n=num_images_for_id, random_state=42)\n",
    "        selected_rows.extend(sampled_rows.values)\n",
    "        num_selected_images += num_images_for_id\n",
    "        \n",
    "        if num_selected_images >= total_img:\n",
    "            break\n",
    "    \n",
    "    # Create a new DataFrame from the selected rows\n",
    "    new_df = pd.DataFrame(selected_rows, columns=['image_name', 'id', 'race'])\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced(data, image_per_id_limit=60, total_image=120000, id_per_race=500): \n",
    "    race_to_label = {'Caucasian': 0, 'Indian': 1, 'Asian': 2, 'African': 3}\n",
    "    balanced_data = pd.DataFrame()\n",
    "\n",
    "    for race in race_to_label.keys():\n",
    "        race_label = race_to_label[race]\n",
    "        cur_race_data = data[data['race']==race_label]\n",
    "        race_balanced_df = get_balanced_per_race(cur_race_data, image_per_id_limit, total_image, id_per_race)\n",
    "        balanced_data = pd.concat([balanced_data, race_balanced_df])\n",
    "    \n",
    "    return balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"all_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data = get_balanced(data, 60, 120000, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n",
      "               image_name        id  race\n",
      "0    0-FaceId-0_align.jpg  m.01q9hh     0\n",
      "1  102-FaceId-0_align.jpg  m.01q9hh     0\n",
      "2   33-FaceId-0_align.jpg  m.01q9hh     0\n",
      "3   14-FaceId-0_align.jpg  m.01q9hh     0\n",
      "4   51-FaceId-0_align.jpg  m.01q9hh     0\n",
      "id\n",
      "m.0100bl7z    60\n",
      "m.0101hx8n    60\n",
      "m.0104r_91    60\n",
      "m.0105jt_f    60\n",
      "m.010887hy    60\n",
      "              ..\n",
      "m.0zmx0h5     60\n",
      "m.0zrryfx     60\n",
      "m.0zs796g     60\n",
      "m.0ztd97b     60\n",
      "m.0ztdp_f     60\n",
      "Name: image_name, Length: 2000, dtype: int64\n",
      "race\n",
      "0    30000\n",
      "1    30000\n",
      "2    30000\n",
      "3    30000\n",
      "Name: image_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "balanced_data = pd.read_csv(\"./balanced_data.csv\")\n",
    "print(len(balanced_data))\n",
    "print(balanced_data.head())\n",
    "print(balanced_data.groupby('id').count()['image_name'])\n",
    "print(balanced_data.groupby('race').count()['image_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv(balanced_data, \"./\", \"balanced_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Prob Dataset\n",
    "- Linear prob: 3-10 id/race, 40 id in total, 60 img/id, == 2.4k image. 50 img/id trainâ€¦ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sample 10 ids from each race, each of them already have 60 img/race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def sample_linear_prob(original_balanced_data):\n",
    "    # Load your dataset\n",
    "    original_balanced_data = pd.read_csv(\"./balanced_data.csv\")\n",
    "\n",
    "    # Initialize an empty DataFrame to store the sampled data\n",
    "    sampled_data = pd.DataFrame()\n",
    "\n",
    "    # Get unique races\n",
    "    races = original_balanced_data['race'].unique()\n",
    "\n",
    "    # For each race, randomly sample 10 ids and add them to sampled_data\n",
    "    for race in races:\n",
    "        sampled_ids = original_balanced_data[original_balanced_data['race'] == race]['id'].drop_duplicates().sample(10)\n",
    "        sampled_data = pd.concat([sampled_data, original_balanced_data[original_balanced_data['id'].isin(sampled_ids)]])\n",
    "\n",
    "    # Remove the sampled ids from original_balanced_data\n",
    "    new_balanced_data = original_balanced_data[~original_balanced_data['id'].isin(sampled_data['id'])]\n",
    "    return sampled_data, new_balanced_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_balanced_data = pd.read_csv(\"./balanced_data.csv\")\n",
    "\n",
    "linear_prob_dataset,  new_balanced_data= sample_linear_prob(original_balanced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Filter Linear Prob ids from Old Uniform to create New Uniform\n",
    "    - Should contain 118k images\n",
    "    - Check for data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no overlap between linear_prob_dataset and the new balanced data.\n",
      "Number of identity overlap between linear_prob_dataset and balanced data:  0\n"
     ]
    }
   ],
   "source": [
    "check_data_leakage(new_balanced_data, {'linear_prob_dataset': linear_prob_dataset})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SAVE linear prob and new balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv(linear_prob_dataset, \"./\", \"linear_prob_dataset.csv\")\n",
    "save_csv(balanced_data, \"./\", \"new_balanced_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out balanced data and get disjoint_balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data = pd.read_csv(\"./new_balanced_data.csv\")\n",
    "data = pd.read_csv(\"./all_data.csv\")\n",
    "#filter out everything in balanced_data\n",
    "filtered_data = data[~data['id'].isin(balanced_data['id'])]\n",
    "\n",
    "#filter out id with less than 20 images\n",
    "filtered_data = filtered_data.groupby('id').filter(lambda x: len(x) >= 20)\n",
    "\n",
    "#check number of id per race\n",
    "id_per_race = filtered_data.groupby('race')['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117600"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(balanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv(filtered_data, \"./\", \"disjoint_balanced_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample biased datasets for each race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sample_by_race(race_data, id_per_race, image_per_id):\n",
    "    total_img = id_per_race * image_per_id\n",
    "\n",
    "    # Group by ID and count the number of images per ID\n",
    "    data_grouped = race_data.groupby(\"id\").count()\n",
    "    \n",
    "    # Get a list of IDs that meet the image count criteria\n",
    "    eligible_ids = data_grouped.index.tolist()\n",
    "    \n",
    "    # Shuffle the list of eligible IDs\n",
    "    random.shuffle(eligible_ids)\n",
    "    \n",
    "    # Take the first 'id_per_race' IDs to ensure uniform distribution\n",
    "    selected_ids = eligible_ids[:id_per_race]\n",
    "    \n",
    "    # Filter the DataFrame to include only rows with selected IDs\n",
    "    filtered_id_df = race_data[race_data[\"id\"].isin(selected_ids)]\n",
    "    \n",
    "    # Randomly sample images from each selected ID to meet the 'total_img' requirement\n",
    "    num_selected_images = 0\n",
    "    selected_rows = []\n",
    "    \n",
    "    for id in tqdm(selected_ids, desc=\"Sampling Images\"):\n",
    "        id_df = filtered_id_df[filtered_id_df[\"id\"] == id]\n",
    "        num_images_for_id = min(image_per_id, total_img - num_selected_images)\n",
    "        \n",
    "        # Randomly sample 'num_images_for_id' images for the current ID\n",
    "        sampled_rows = id_df.sample(n=num_images_for_id, random_state=42)\n",
    "        selected_rows.extend(sampled_rows.values)\n",
    "        num_selected_images += num_images_for_id\n",
    "        \n",
    "        if num_selected_images >= total_img:\n",
    "            break\n",
    "    \n",
    "    # Create a new DataFrame from the selected rows\n",
    "    new_df = pd.DataFrame(selected_rows, columns=['image_name', 'id', 'race'])\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def generate_unbalanced_datasets(filtered_csv, output_dir, majority_num_ids=3600, minority_num_ids=780, image_per_id=20):\n",
    "    race_to_label = {'Caucasian': 0, 'Indian': 1, 'Asian': 2, 'African': 3}\n",
    "    label_to_race = {0: 'Caucasian', 1: 'Indian', 2: 'Asian', 3: 'African'}\n",
    "    \n",
    "    filtered_data = pd.read_csv(filtered_csv)\n",
    "    coloumns = ['image_name', 'id', 'race']\n",
    "\n",
    "    for race, label in race_to_label.items():\n",
    "        print(f\"Race: {race}\")\n",
    "        print(f\"Label: {label}\")\n",
    "\n",
    "        cur_label = label\n",
    "        cur_biased_data = pd.DataFrame(columns=coloumns)\n",
    "\n",
    "        for iter_label, _ in label_to_race.items():\n",
    "            print(f\"iter_label: {iter_label}\")\n",
    "            if iter_label == cur_label:\n",
    "                majority_race = filtered_data[filtered_data['race'] == cur_label]\n",
    "                majority_sample = sample_by_race(majority_race, majority_num_ids, image_per_id)\n",
    "                cur_biased_data = pd.concat([cur_biased_data, majority_sample])\n",
    "            else:\n",
    "                minority_sample = filtered_data[filtered_data['race'] == iter_label]\n",
    "                minority_sample = sample_by_race(minority_sample, minority_num_ids, image_per_id)\n",
    "                cur_biased_data = pd.concat([cur_biased_data, minority_sample])\n",
    "\n",
    "        # Save the unbalanced data as CSV.\n",
    "        cur_biased_data.to_csv(f'{output_dir}/unbalanced_{race}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race: Caucasian\n",
      "Label: 0\n",
      "iter_label: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3599/3600 [01:41<00:00, 35.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 779/780 [00:06<00:00, 123.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_label: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 779/780 [00:05<00:00, 130.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_label: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 779/780 [00:08<00:00, 89.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race: Indian\n",
      "Label: 1\n",
      "iter_label: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 779/780 [00:11<00:00, 66.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3599/3600 [01:58<00:00, 30.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_label: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 779/780 [00:06<00:00, 122.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_label: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 779/780 [00:08<00:00, 92.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race: Asian\n",
      "Label: 2\n",
      "iter_label: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 779/780 [00:07<00:00, 101.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 779/780 [00:09<00:00, 84.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_label: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3599/3600 [02:00<00:00, 29.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_label: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 779/780 [00:07<00:00, 108.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race: African\n",
      "Label: 3\n",
      "iter_label: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 779/780 [00:08<00:00, 95.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 779/780 [00:06<00:00, 115.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_label: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 779/780 [00:06<00:00, 112.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_label: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3599/3600 [02:03<00:00, 29.21it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_unbalanced_datasets(\"./disjoint_balanced_data.csv\", \"./\", majority_num_ids=3600, minority_num_ids=780, image_per_id=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "african_unbalnced = pd.read_csv(\"./unbalanced_African.csv\")\n",
    "asian_unbalnced = pd.read_csv(\"./unbalanced_Asian.csv\")\n",
    "caucasian_unbalnced = pd.read_csv(\"./unbalanced_Caucasian.csv\")\n",
    "indian_unbalnced = pd.read_csv(\"./unbalanced_Indian.csv\")\n",
    "balanced_data = pd.read_csv(\"./new_balanced_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_name\n",
      "20    5940\n",
      "Name: count, dtype: int64\n",
      "image_name\n",
      "20    5940\n",
      "Name: count, dtype: int64\n",
      "Caucasian unbalanced image_name\n",
      "20    5940\n",
      "Name: count, dtype: int64\n",
      "Indian unbalanced image_name\n",
      "20    5940\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(african_unbalnced.groupby('id').count()['image_name'].value_counts())\n",
    "print(asian_unbalnced.groupby('id').count()['image_name'].value_counts())\n",
    "print(\"Caucasian unbalanced\", caucasian_unbalnced.groupby('id').count()['image_name'].value_counts())\n",
    "print(\"Indian unbalanced\", indian_unbalnced.groupby('id').count()['image_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "race\n",
       "0     780\n",
       "1     780\n",
       "2     780\n",
       "3    3600\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "african_unbalnced.groupby('race')['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of identity overlap between african_unbalnced and balanced_data:  0\n",
      "number of identity overlap between asian_unbalnced and balanced_data:  0\n",
      "number of identity overlap between caucasian_unbalnced and balanced_data:  0\n",
      "number of identity overlap between indian_unbalnced and balanced_data:  0\n"
     ]
    }
   ],
   "source": [
    "#check identity leakage\n",
    "african_unbalnced_id = african_unbalnced['id'].tolist()\n",
    "asian_unbalnced_id = asian_unbalnced['id'].tolist()\n",
    "caucasian_unbalnced_id = caucasian_unbalnced['id'].tolist()\n",
    "indian_unbalnced_id = indian_unbalnced['id'].tolist()\n",
    "\n",
    "balanced_data_id = balanced_data['id'].tolist()\n",
    "\n",
    "print(\"number of identity overlap between african_unbalnced and balanced_data: \", len(set(african_unbalnced_id).intersection(balanced_data_id)))\n",
    "print(\"number of identity overlap between asian_unbalnced and balanced_data: \", len(set(asian_unbalnced_id).intersection(balanced_data_id)))\n",
    "print(\"number of identity overlap between caucasian_unbalnced and balanced_data: \", len(set(caucasian_unbalnced_id).intersection(balanced_data_id)))\n",
    "print(\"number of identity overlap between indian_unbalnced and balanced_data: \", len(set(indian_unbalnced_id).intersection(balanced_data_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Train and Test in Balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_train_test(data, percentage_test_per_id=0.2):\n",
    "    image_per_id = data.groupby('id').count()['image_name'][0]\n",
    "    print(f\"image_per_id: {image_per_id}\")\n",
    "    num_test_per_id = (image_per_id * percentage_test_per_id).astype(int)\n",
    "    print(f\"num_test_per_id: {num_test_per_id}\")\n",
    "\n",
    "    result_df = data.copy()\n",
    "    result_df['split'] = 'train'\n",
    "    print(f\"result_df: {result_df}\")\n",
    "\n",
    "    unique_ids = data['id'].unique()\n",
    "    print(f\"len(unique_ids): {len(unique_ids)}\")\n",
    "\n",
    "    # return\n",
    "    for id in unique_ids:\n",
    "        test_indices = data[data['id'] == id].head(num_test_per_id).index\n",
    "        result_df.loc[test_indices, 'split'] = 'test'\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'train' images: 48\n",
      "Number of 'test' images: 12\n"
     ]
    }
   ],
   "source": [
    "data = \"\"\"000020_00@en.jpg,m.0j5bv6r,1,test\n",
    "000000_00@fa.jpg,m.0j5bv6r,1,test\n",
    "000001_00@fa.jpg,m.0j5bv6r,1,test\n",
    "000048_00@fa.jpg,m.0j5bv6r,1,test\n",
    "000006_00@en.jpg,m.0j5bv6r,1,test\n",
    "000018_00@fa.jpg,m.0j5bv6r,1,test\n",
    "000044_00@en.jpg,m.0j5bv6r,1,test\n",
    "000066_00@en.jpg,m.0j5bv6r,1,test\n",
    "000021_00@fa.jpg,m.0j5bv6r,1,test\n",
    "000006_00@fa.jpg,m.0j5bv6r,1,test\n",
    "000059_00@en.jpg,m.0j5bv6r,1,test\n",
    "000005_01@en.jpg,m.0j5bv6r,1,test\n",
    "000058_00@en.jpg,m.0j5bv6r,1,train\n",
    "000026_00@en.jpg,m.0j5bv6r,1,train\n",
    "000003_00@en.jpg,m.0j5bv6r,1,train\n",
    "000061_00@en.jpg,m.0j5bv6r,1,train\n",
    "000024_00@fa.jpg,m.0j5bv6r,1,train\n",
    "000025_00@en.jpg,m.0j5bv6r,1,train\n",
    "000028_00@en.jpg,m.0j5bv6r,1,train\n",
    "000069_00@en.jpg,m.0j5bv6r,1,train\n",
    "000049_00@fa.jpg,m.0j5bv6r,1,train\n",
    "000060_00@en.jpg,m.0j5bv6r,1,train\n",
    "000014_00@en.jpg,m.0j5bv6r,1,train\n",
    "000037_00@en.jpg,m.0j5bv6r,1,train\n",
    "000007_00@fa.jpg,m.0j5bv6r,1,train\n",
    "000055_00@en.jpg,m.0j5bv6r,1,train\n",
    "000054_00@en.jpg,m.0j5bv6r,1,train\n",
    "000025_00@fa.jpg,m.0j5bv6r,1,train\n",
    "000027_00@en.jpg,m.0j5bv6r,1,train\n",
    "000013_00@en.jpg,m.0j5bv6r,1,train\n",
    "000004_00@en.jpg,m.0j5bv6r,1,train\n",
    "000045_01@en.jpg,m.0j5bv6r,1,train\n",
    "000021_00@en.jpg,m.0j5bv6r,1,train\n",
    "000002_00@en.jpg,m.0j5bv6r,1,train\n",
    "000067_00@en.jpg,m.0j5bv6r,1,train\n",
    "000068_00@en.jpg,m.0j5bv6r,1,train\n",
    "000022_00@en.jpg,m.0j5bv6r,1,train\n",
    "000036_01@en.jpg,m.0j5bv6r,1,train\n",
    "000000_01@en.jpg,m.0j5bv6r,1,train\n",
    "000012_00@fa.jpg,m.0j5bv6r,1,train\n",
    "000047_00@en.jpg,m.0j5bv6r,1,train\n",
    "000038_00@en.jpg,m.0j5bv6r,1,train\n",
    "000009_01@en.jpg,m.0j5bv6r,1,train\n",
    "000045_00@fa.jpg,m.0j5bv6r,1,train\n",
    "000033_00@fa.jpg,m.0j5bv6r,1,train\n",
    "000017_00@fa.jpg,m.0j5bv6r,1,train\n",
    "000035_00@en.jpg,m.0j5bv6r,1,train\n",
    "000032_00@en.jpg,m.0j5bv6r,1,train\n",
    "000008_00@en.jpg,m.0j5bv6r,1,train\n",
    "000071_01@en.jpg,m.0j5bv6r,1,train\n",
    "000016_00@fa.jpg,m.0j5bv6r,1,train\n",
    "000024_00@en.jpg,m.0j5bv6r,1,train\n",
    "000049_00@en.jpg,m.0j5bv6r,1,train\n",
    "000015_00@en.jpg,m.0j5bv6r,1,train\n",
    "000040_00@en.jpg,m.0j5bv6r,1,train\n",
    "000039_00@en.jpg,m.0j5bv6r,1,train\n",
    "000023_00@en.jpg,m.0j5bv6r,1,train\n",
    "000057_00@en.jpg,m.0j5bv6r,1,train\n",
    "000002_00@fa.jpg,m.0j5bv6r,1,train\n",
    "000005_00@en.jpg,m.0j5bv6r,1,train\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into lines\n",
    "lines = data.strip().split('\\n')\n",
    "\n",
    "# Create counters for 'train' and 'test' splits\n",
    "train_count = 0\n",
    "test_count = 0\n",
    "\n",
    "# Iterate through the lines and count 'train' and 'test' images\n",
    "for line in lines:\n",
    "    _, _, _, split = line.split(',')\n",
    "    if split == 'train':\n",
    "        train_count += 1\n",
    "    elif split == 'test':\n",
    "        test_count += 1\n",
    "\n",
    "print(f\"Number of 'train' images: {train_count}\")\n",
    "print(f\"Number of 'test' images: {test_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b4/lw4c28nj0t9673hql45sbnp80000gn/T/ipykernel_98387/218188292.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  image_per_id = balanced_data.groupby('id').count()['image_name'][0]\n"
     ]
    }
   ],
   "source": [
    "#ignor the warning, it works fine \n",
    "race_to_label = {'Caucasian': 0, 'Indian': 1, 'Asian': 2, 'African': 3}\n",
    "label_to_race = {0: 'Caucasian', 1: 'Indian', 2: 'Asian', 3: 'African'}\n",
    "\n",
    "balanced_data = pd.read_csv(\"./unbalanced_African.csv\")\n",
    "african_balanced = balanced_data[balanced_data['race'] == 3]\n",
    "african_balanced_split = mark_train_test(african_balanced, 0.2)\n",
    "\n",
    "save_csv(african_balanced_split, \"./\", \"african_balanced_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_per_id: 60\n",
      "num_test_per_id: 12\n",
      "result_df:                     image_name        id  race  split\n",
      "0         0-FaceId-0_align.jpg  m.01q9hh     0  train\n",
      "1       102-FaceId-0_align.jpg  m.01q9hh     0  train\n",
      "2        33-FaceId-0_align.jpg  m.01q9hh     0  train\n",
      "3        14-FaceId-0_align.jpg  m.01q9hh     0  train\n",
      "4        51-FaceId-0_align.jpg  m.01q9hh     0  train\n",
      "...                        ...       ...   ...    ...\n",
      "117595    8-FaceId-0_align.jpg  m.0fn4cg     3  train\n",
      "117596   40-FaceId-0_align.jpg  m.0fn4cg     3  train\n",
      "117597   37-FaceId-0_align.jpg  m.0fn4cg     3  train\n",
      "117598    0-FaceId-0_align.jpg  m.0fn4cg     3  train\n",
      "117599   41-FaceId-0_align.jpg  m.0fn4cg     3  train\n",
      "\n",
      "[117600 rows x 4 columns]\n",
      "len(unique_ids): 1960\n"
     ]
    }
   ],
   "source": [
    "# Define the race-to-label and label-to-race mappings\n",
    "race_to_label = {'Caucasian': 0, 'Indian': 1, 'Asian': 2, 'African': 3}\n",
    "label_to_race = {0: 'Caucasian', 1: 'Indian', 2: 'Asian', 3: 'African'}\n",
    "\n",
    "# Read your balanced data from a CSV file\n",
    "balanced_data = pd.read_csv(\"./new_balanced_data.csv\")\n",
    "\n",
    "# Create an empty dictionary to store splits by race\n",
    "race_splits = {}\n",
    "\n",
    "# Iterate through races and split the data\n",
    "for race, label in race_to_label.items():\n",
    "    race_data = balanced_data[balanced_data['race'] == label]\n",
    "    \n",
    "    # Assuming you have a function mark_train_test to split the data\n",
    "    race_split = mark_train_test(balanced_data, percentage_test_per_id=0.2)\n",
    "    break\n",
    "    # Save the split into a CSV file\n",
    "    save_csv(race_split, \"./\", f\"{race}_balanced_split.csv\")\n",
    "\n",
    "    # Store the split in the dictionary\n",
    "    race_splits[race] = race_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7840"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1960 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Define a function to split each group\n",
    "def split_data(group):\n",
    "    train, test = train_test_split(group, test_size=0.2, random_state=42)\n",
    "    return pd.concat([train, test], keys=['train', 'test'], names=['split'])\n",
    "\n",
    "# Apply the function to each group\n",
    "data = data.groupby('id').apply(split_data).reset_index(level='split')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race: Caucasian\n",
      "image_per_id: 20\n",
      "num_test_per_id: 4\n",
      "result_df:                     image_name         id  race  split\n",
      "0        28-FaceId-1_align.jpg  m.0g550l7     0  train\n",
      "1         5-FaceId-0_align.jpg  m.0g550l7     0  train\n",
      "2         3-FaceId-0_align.jpg  m.0g550l7     0  train\n",
      "3        15-FaceId-0_align.jpg  m.0g550l7     0  train\n",
      "4        47-FaceId-0_align.jpg  m.0g550l7     0  train\n",
      "...                        ...        ...   ...    ...\n",
      "118795  105-FaceId-0_align.jpg   m.02pyk8     3  train\n",
      "118796  102-FaceId-0_align.jpg   m.02pyk8     3  train\n",
      "118797   89-FaceId-0_align.jpg   m.02pyk8     3  train\n",
      "118798   35-FaceId-0_align.jpg   m.02pyk8     3  train\n",
      "118799   21-FaceId-0_align.jpg   m.02pyk8     3  train\n",
      "\n",
      "[118800 rows x 4 columns]\n",
      "len(unique_ids): 5940\n",
      "race: Indian\n",
      "image_per_id: 20\n",
      "num_test_per_id: 4\n",
      "result_df:                     image_name         id  race  split\n",
      "0        14-FaceId-1_align.jpg  m.01v2gc6     0  train\n",
      "1        13-FaceId-0_align.jpg  m.01v2gc6     0  train\n",
      "2        38-FaceId-0_align.jpg  m.01v2gc6     0  train\n",
      "3         2-FaceId-0_align.jpg  m.01v2gc6     0  train\n",
      "4        36-FaceId-0_align.jpg  m.01v2gc6     0  train\n",
      "...                        ...        ...   ...    ...\n",
      "118795   93-FaceId-0_align.jpg   m.04bxj3     3  train\n",
      "118796   68-FaceId-1_align.jpg   m.04bxj3     3  train\n",
      "118797   18-FaceId-0_align.jpg   m.04bxj3     3  train\n",
      "118798  108-FaceId-0_align.jpg   m.04bxj3     3  train\n",
      "118799   74-FaceId-0_align.jpg   m.04bxj3     3  train\n",
      "\n",
      "[118800 rows x 4 columns]\n",
      "len(unique_ids): 5940\n",
      "race: Asian\n",
      "image_per_id: 20\n",
      "num_test_per_id: 4\n",
      "result_df:                    image_name         id  race  split\n",
      "0       27-FaceId-0_align.jpg  m.02r59b1     0  train\n",
      "1       25-FaceId-0_align.jpg  m.02r59b1     0  train\n",
      "2       33-FaceId-0_align.jpg  m.02r59b1     0  train\n",
      "3        7-FaceId-0_align.jpg  m.02r59b1     0  train\n",
      "4       86-FaceId-0_align.jpg  m.02r59b1     0  train\n",
      "...                       ...        ...   ...    ...\n",
      "118795  17-FaceId-0_align.jpg  m.0h9465k     3  train\n",
      "118796  44-FaceId-0_align.jpg  m.0h9465k     3  train\n",
      "118797  31-FaceId-0_align.jpg  m.0h9465k     3  train\n",
      "118798  99-FaceId-0_align.jpg  m.0h9465k     3  train\n",
      "118799  10-FaceId-0_align.jpg  m.0h9465k     3  train\n",
      "\n",
      "[118800 rows x 4 columns]\n",
      "len(unique_ids): 5940\n",
      "race: African\n",
      "image_per_id: 20\n",
      "num_test_per_id: 4\n",
      "result_df:                    image_name         id  race  split\n",
      "0       42-FaceId-0_align.jpg  m.0h1cg90     0  train\n",
      "1       10-FaceId-0_align.jpg  m.0h1cg90     0  train\n",
      "2       77-FaceId-0_align.jpg  m.0h1cg90     0  train\n",
      "3       62-FaceId-0_align.jpg  m.0h1cg90     0  train\n",
      "4       92-FaceId-0_align.jpg  m.0h1cg90     0  train\n",
      "...                       ...        ...   ...    ...\n",
      "118795  33-FaceId-0_align.jpg   m.04znp2     3  train\n",
      "118796  42-FaceId-0_align.jpg   m.04znp2     3  train\n",
      "118797  24-FaceId-0_align.jpg   m.04znp2     3  train\n",
      "118798  93-FaceId-1_align.jpg   m.04znp2     3  train\n",
      "118799  51-FaceId-0_align.jpg   m.04znp2     3  train\n",
      "\n",
      "[118800 rows x 4 columns]\n",
      "len(unique_ids): 5940\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the race-to-label and label-to-race mappings\n",
    "race_to_label = {'Caucasian': 0, 'Indian': 1, 'Asian': 2, 'African': 3}\n",
    "\n",
    "# Load unbalanced datasets\n",
    "african_unbalanced = pd.read_csv(\"./unbalanced_African.csv\")\n",
    "asian_unbalanced = pd.read_csv(\"./unbalanced_Asian.csv\")\n",
    "caucasian_unbalanced = pd.read_csv(\"./unbalanced_Caucasian.csv\")\n",
    "indian_unbalanced = pd.read_csv(\"./unbalanced_Indian.csv\")\n",
    "\n",
    "# Create an empty dictionary to store splits by race for unbalanced datasets\n",
    "unbalanced_race_splits = {}\n",
    "len(f\"african_unbalanced: {african_unbalanced}\")\n",
    "len(f\"asian_unbalanced: {asian_unbalanced}\")\n",
    "len(f\"caucasian_unbalanced: {caucasian_unbalanced}\")\n",
    "len(f\"indian_unbalanced: {indian_unbalanced}\")\n",
    "\n",
    "\n",
    "# Iterate through races and split the unbalanced data\n",
    "for race, _ in race_to_label.items():\n",
    "    if race == 'African':\n",
    "        race_data = african_unbalanced\n",
    "    elif race == 'Asian':\n",
    "        race_data = asian_unbalanced\n",
    "    elif race == 'Caucasian':\n",
    "        race_data = caucasian_unbalanced\n",
    "    elif race == 'Indian':\n",
    "        race_data = indian_unbalanced\n",
    "    \n",
    "    # Assuming you have a function mark_train_test to split the data\n",
    "    print(f\"race: {race}\")\n",
    "    race_split = mark_train_test(race_data, percentage_test_per_id=0.2)\n",
    "    # break\n",
    "    # Save the split into a CSV file\n",
    "    save_csv(race_split, \"./\", f\"unbalanced_{race}_split.csv\")\n",
    "\n",
    "    # Store the split in the dictionary\n",
    "    unbalanced_race_splits[race] = race_split\n",
    "\n",
    "# Now you have splits for each race within the unbalanced datasets stored in unbalanced_race_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "african_unbalanced len: 118800\n",
      "asian_unbalanced len: 118800\n",
      "caucasian_unbalanced len: 118800\n",
      "indian_unbalanced len: 118800\n"
     ]
    }
   ],
   "source": [
    "print(f\"african_unbalanced len: {len(african_unbalanced)}\")\n",
    "print(f\"asian_unbalanced len: {len(asian_unbalanced)}\")\n",
    "print(f\"caucasian_unbalanced len: {len(caucasian_unbalanced)}\")\n",
    "print(f\"indian_unbalanced len: {len(indian_unbalanced)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race: Caucasian, Total len: 29400\n",
      "Race: Caucasian, Train len: 23520, Test len: 5880\n",
      "Race: Indian, Total len: 29400\n",
      "Race: Indian, Train len: 23520, Test len: 5880\n",
      "Race: Asian, Total len: 29400\n",
      "Race: Asian, Train len: 23520, Test len: 5880\n",
      "Race: African, Total len: 29400\n",
      "Race: African, Train len: 23520, Test len: 5880\n"
     ]
    }
   ],
   "source": [
    "def count_unique_ids(train_data, test_data):\n",
    "    train_unique_ids = train_data['id'].nunique()\n",
    "    test_unique_ids = test_data['id'].nunique()\n",
    "    return train_unique_ids, test_unique_ids\n",
    "# print(race_splits)\n",
    "# For balanced dataset\n",
    "for race, split_data in race_splits.items():\n",
    "    train_data = split_data[split_data['split'] == 'train']\n",
    "    test_data = split_data[split_data['split'] == 'test']\n",
    "    train_unique_ids, test_unique_ids = count_unique_ids(train_data, test_data)\n",
    "    # print(f'Race: {race}, Train Unique IDs: {train_unique_ids}, Test Unique IDs: {test_unique_ids}')\n",
    "    print(f'Race: {race}, Total len: {len(split_data)}')\n",
    "    print(f'Race: {race}, Train len: {len(train_data)}, Test len: {len(test_data)}')\n",
    "\n",
    "\n",
    "\n",
    "# For unbalanced datasets\n",
    "for race, split_data in unbalanced_race_splits.items():\n",
    "    train_data = split_data[split_data['split'] == 'train']\n",
    "    test_data = split_data[split_data['split'] == 'test']\n",
    "    train_unique_ids, test_unique_ids = count_unique_ids(train_data, test_data)\n",
    "    # print(f'Race: {race}, Train Unique IDs: {train_unique_ids}, Test Unique IDs: {test_unique_ids}')\n",
    "    print(f'Race: {race}, Total len: {len(split_data)}')\n",
    "    print(f'Race: {race}, Train len: {len(train_data)}, Test len: {len(test_data)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "71280>47520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5880 / 29400 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "29400 > 118800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1200"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "29400 * 4 - 118800 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lorenzomendoza/Documents/CS_Courses/Current/CSC_277/project/SupContrast-ORE/data/get_data.ipynb Cell 44\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzomendoza/Documents/CS_Courses/Current/CSC_277/project/SupContrast-ORE/data/get_data.ipynb#Y342sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m train_data[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzomendoza/Documents/CS_Courses/Current/CSC_277/project/SupContrast-ORE/data/get_data.ipynb#Y342sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         train_ratio \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_data[train_data[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mid\u001b[39m]) \u001b[39m/\u001b[39m total_ids\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lorenzomendoza/Documents/CS_Courses/Current/CSC_277/project/SupContrast-ORE/data/get_data.ipynb#Y342sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         test_ratio \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(test_data[test_data[\u001b[39m'\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m \u001b[39mid\u001b[39;49m]) \u001b[39m/\u001b[39m total_ids\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzomendoza/Documents/CS_Courses/Current/CSC_277/project/SupContrast-ORE/data/get_data.ipynb#Y342sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         id_ratios\u001b[39m.\u001b[39mappend([\u001b[39mid\u001b[39m, race, train_ratio, test_ratio])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzomendoza/Documents/CS_Courses/Current/CSC_277/project/SupContrast-ORE/data/get_data.ipynb#Y342sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# For unbalanced datasets\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/pandas/core/ops/common.py:81\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     79\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__eq__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__eq__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cmp_method(other, operator\u001b[39m.\u001b[39;49meq)\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/pandas/core/series.py:6096\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6093\u001b[0m rvalues \u001b[39m=\u001b[39m extract_array(other, extract_numpy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, extract_range\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   6095\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 6096\u001b[0m     res_values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mcomparison_op(lvalues, rvalues, op)\n\u001b[1;32m   6098\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(res_values, name\u001b[39m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:293\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[39mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    292\u001b[0m \u001b[39melif\u001b[39;00m is_object_dtype(lvalues\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(rvalues, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     res_values \u001b[39m=\u001b[39m comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[1;32m    295\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     res_values \u001b[39m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/machinelearning/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:82\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     80\u001b[0m     result \u001b[39m=\u001b[39m libops\u001b[39m.\u001b[39mvec_compare(x\u001b[39m.\u001b[39mravel(), y\u001b[39m.\u001b[39mravel(), op)\n\u001b[1;32m     81\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     result \u001b[39m=\u001b[39m libops\u001b[39m.\u001b[39;49mscalar_compare(x\u001b[39m.\u001b[39;49mravel(), y, op)\n\u001b[1;32m     83\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to collect the data\n",
    "id_ratios = []\n",
    "\n",
    "# For balanced dataset\n",
    "for race, split_data in race_splits.items():\n",
    "    # Calculate ratio for each ID\n",
    "    total_ids = len(train_data['id'].unique()) + len(test_data['id'].unique())\n",
    "    \n",
    "    for id in train_data['id'].unique():\n",
    "        train_ratio = len(train_data[train_data['id'] == id]) / total_ids\n",
    "        test_ratio = len(test_data[test_data['id'] == id]) / total_ids\n",
    "        id_ratios.append([id, race, train_ratio, test_ratio])\n",
    "\n",
    "# For unbalanced datasets\n",
    "for race, split_data in unbalanced_race_splits.items():\n",
    "    # Calculate ratio for each ID\n",
    "    total_ids = len(train_data['id'].unique()) + len(test_data['id'].unique())\n",
    "    \n",
    "    for id in train_data['id'].unique():\n",
    "        train_ratio = len(train_data[train_data['id'] == id]) / total_ids\n",
    "        test_ratio = len(test_data[test_data['id'] == id]) / total_ids\n",
    "        id_ratios.append([id, race, train_ratio, test_ratio])\n",
    "\n",
    "# Create a DataFrame from the list of ratios\n",
    "id_ratio_df = pd.DataFrame(id_ratios, columns=['ID', 'Race', 'Train Ratio', 'Test Ratio'])\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(id_ratio_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Prob Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_prob_dataset = pd.read_csv(\"./linear_prob_dataset.csv\")\n",
    "linear_prob_dataset_split = mark_train_test(linear_prob_dataset, 0.2)\n",
    "\n",
    "save_csv(linear_prob_dataset_split, \"./\", \"linear_prob_dataset_split.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race: 0\n",
      "Total Unique IDs: 10\n",
      "Train Unique IDs: 10\n",
      "Test Unique IDs: 10\n",
      "Race: 1\n",
      "Total Unique IDs: 10\n",
      "Train Unique IDs: 10\n",
      "Test Unique IDs: 10\n",
      "Race: 2\n",
      "Total Unique IDs: 10\n",
      "Train Unique IDs: 10\n",
      "Test Unique IDs: 10\n",
      "Race: 3\n",
      "Total Unique IDs: 10\n",
      "Train Unique IDs: 10\n",
      "Test Unique IDs: 10\n",
      "\n",
      "Overall Statistics:\n",
      "Total Unique IDs: 40\n",
      "Train Unique IDs: 10 (100.00%)\n",
      "Test Unique IDs: 10 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# Load the 'linear_prob_dataset_split' dataset\n",
    "linear_prob_dataset_split = pd.read_csv(\"linear_prob_dataset_split.csv\")\n",
    "\n",
    "\n",
    "# Create a dictionary to store the counts for each race\n",
    "race_id_counts = {}\n",
    "\n",
    "# Separate the 'train' and 'test' data\n",
    "train_data = linear_prob_dataset_split[linear_prob_dataset_split['split'] == 'train']\n",
    "test_data = linear_prob_dataset_split[linear_prob_dataset_split['split'] == 'test']\n",
    "\n",
    "# Calculate the number of unique IDs and their ratios\n",
    "train_unique_ids, test_unique_ids = count_unique_ids(train_data, test_data)\n",
    "total_unique_ids = len(linear_prob_dataset_split['id'].unique())\n",
    "train_ratio = train_unique_ids / total_unique_ids\n",
    "test_ratio = test_unique_ids / total_unique_ids\n",
    "\n",
    "# Count unique IDs per race\n",
    "for race in linear_prob_dataset_split['race'].unique():\n",
    "    race_data = linear_prob_dataset_split[linear_prob_dataset_split['race'] == race]\n",
    "    race_train_data = train_data[train_data['race'] == race]\n",
    "    race_test_data = test_data[test_data['race'] == race]\n",
    "    \n",
    "    train_unique_ids, test_unique_ids = count_unique_ids(race_train_data, race_test_data)\n",
    "    total_race_ids = len(race_data['id'].unique())\n",
    "    \n",
    "    race_id_counts[race] = {\n",
    "        'Total Unique IDs': total_race_ids,\n",
    "        'Train Unique IDs': train_unique_ids,\n",
    "        'Test Unique IDs': test_unique_ids,\n",
    "    }\n",
    "\n",
    "    # Print results for each race\n",
    "    print(f\"Race: {race}\")\n",
    "    print(f\"Total Unique IDs: {total_race_ids}\")\n",
    "    print(f\"Train Unique IDs: {train_unique_ids}\")\n",
    "    print(f\"Test Unique IDs: {test_unique_ids}\")\n",
    "\n",
    "# Print overall results\n",
    "print(\"\\nOverall Statistics:\")\n",
    "print(f\"Total Unique IDs: {total_unique_ids}\")\n",
    "print(f\"Train Unique IDs: {train_unique_ids} ({train_ratio * 100:.2f}%)\")\n",
    "print(f\"Test Unique IDs: {test_unique_ids} ({test_ratio * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def number_img_per_id(linear_prob_dataset_split):\n",
    "    # Create a dictionary to store the counts for each race\n",
    "    race_id_counts = {}\n",
    "\n",
    "    # Separate the 'train' and 'test' data\n",
    "    train_data = linear_prob_dataset_split[linear_prob_dataset_split['split'] == 'train']\n",
    "    test_data = linear_prob_dataset_split[linear_prob_dataset_split['split'] == 'test']\n",
    "\n",
    "    # Calculate the number of unique IDs and their ratios\n",
    "    total_unique_ids = len(linear_prob_dataset_split['id'].unique())\n",
    "\n",
    "    # Count unique IDs per race\n",
    "    for race in linear_prob_dataset_split['race'].unique():\n",
    "        race_data = linear_prob_dataset_split[linear_prob_dataset_split['race'] == race]\n",
    "        race_train_data = train_data[train_data['race'] == race]\n",
    "        race_test_data = test_data[test_data['race'] == race]\n",
    "\n",
    "        race_id_counts[race] = {'Total Unique IDs': 0, 'ID Image Counts': {}}\n",
    "\n",
    "        for id in race_data['id'].unique():\n",
    "            total_race_images = len(race_data[race_data['id'] == id])\n",
    "            train_race_images = len(race_train_data[race_train_data['id'] == id])\n",
    "            test_race_images = len(race_test_data[race_test_data['id'] == id])\n",
    "\n",
    "            race_id_counts[race]['ID Image Counts'][id] = {\n",
    "                'Total Images': total_race_images,\n",
    "                'Train Images': train_race_images,\n",
    "                'Test Images': test_race_images,\n",
    "            }\n",
    "\n",
    "            # Update the total unique IDs count for the race\n",
    "            race_id_counts[race]['Total Unique IDs'] += 1\n",
    "\n",
    "    # Print results for each race\n",
    "    for race, race_info in race_id_counts.items():\n",
    "        print(f\"Race: {race}\")\n",
    "        print(f\"Total Unique IDs: {race_info['Total Unique IDs']}\")\n",
    "\n",
    "        for id, id_info in race_info['ID Image Counts'].items():\n",
    "            print(f\"ID: {id}\")\n",
    "            print(f\"Total Images: {id_info['Total Images']}\")\n",
    "            print(f\"Train Images: {id_info['Train Images']}\")\n",
    "            print(f\"Test Images: {id_info['Test Images']}\")\n",
    "            print()\n",
    "\n",
    "    # Print overall results\n",
    "    print(\"\\nOverall Statistics:\")\n",
    "    print(f\"Total Unique IDs: {total_unique_ids}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race: 0\n",
      "Total Unique IDs: 10\n",
      "ID: m.07h5rn\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0181j_\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0240pk\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.01lb8z\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.09fdg1\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0cc99yf\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.03tjn_\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.09d6n9\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.020skv\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0bb8pbs\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "Race: 1\n",
      "Total Unique IDs: 10\n",
      "ID: m.04q43q\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0ffgks\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0pcvrt5\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0cfxd5\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.09z1b2\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0bh12n\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0dd9hvn\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0ktdtr\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0cb1h4\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0czmj0\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "Race: 2\n",
      "Total Unique IDs: 10\n",
      "ID: m.0jyvwv\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.01vfm2v\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.025y4kr\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.03c355h\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0hn9wjt\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0jt66jg\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.03gw8ss\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0zg9h1c\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0n476ww\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.04n03h8\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "Race: 3\n",
      "Total Unique IDs: 10\n",
      "ID: m.03bz141\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0fxr_v\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0chd66\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0437ps\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0415n32\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.02ryn_3\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.03bz8l9\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.05km3r\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.04yy7s\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "ID: m.0406m8s\n",
      "Total Images: 60\n",
      "Train Images: 48\n",
      "Test Images: 12\n",
      "\n",
      "\n",
      "Overall Statistics:\n",
      "Total Unique IDs: 40\n"
     ]
    }
   ],
   "source": [
    "number_img_per_id(linear_prob_dataset_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save train and test image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_train_image(identity_selection, data_path, save_path, label_to_race):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    # Make a copy of the identity_selection\n",
    "    identity_selection = identity_selection.copy()\n",
    "    if 'split' in identity_selection.columns:\n",
    "        identity_selection = identity_selection[identity_selection['split'] == 'train']\n",
    "\n",
    "    for _, row in tqdm(identity_selection.iterrows(), total=len(identity_selection), desc=\"Processing Images\"):\n",
    "        image_name = row['image_name']\n",
    "        id = row['id']\n",
    "        race = label_to_race[row['race']]\n",
    "\n",
    "        source_path = os.path.join(data_path, race, id, image_name)\n",
    "        identity_save_path = os.path.join(save_path, id)\n",
    "\n",
    "        # Create a subdirectory for the identity if it doesn't exist\n",
    "        if not os.path.exists(identity_save_path):\n",
    "            os.makedirs(identity_save_path)\n",
    "\n",
    "        destination_path = os.path.join(identity_save_path, image_name)\n",
    "        \n",
    "        # Check if the image already exists\n",
    "        if os.path.exists(destination_path):\n",
    "            continue\n",
    "\n",
    "        #check if the source image exists\n",
    "        if not os.path.exists(source_path):\n",
    "            print(\"{} does not exist\".format(source_path))\n",
    "            continue\n",
    "\n",
    "        # Copy the image without converting\n",
    "        shutil.copy(source_path, destination_path)\n",
    "\n",
    "    print('Finished copying training images to folders in JPG format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_image(identity_selection, data_path, save_path, label_to_race):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)    \n",
    "\n",
    "    identity_selection = identity_selection.copy()\n",
    "    \n",
    "    if 'split' in identity_selection.columns:\n",
    "        identity_selection = identity_selection[identity_selection['split'] == 'test']\n",
    "\n",
    "    for _, row in tqdm(identity_selection.iterrows(), total=len(identity_selection), desc=\"Processing Images\"):\n",
    "        image_name = row['image_name']\n",
    "        id = row['id']\n",
    "        race = label_to_race[row['race']]\n",
    "\n",
    "        source_path = os.path.join(data_path, race, id, image_name)\n",
    "\n",
    "        # Save to each race folder\n",
    "        race_path = os.path.join(save_path, race)\n",
    "        if not os.path.exists(race_path):\n",
    "            os.makedirs(race_path)\n",
    "\n",
    "        identity_save_path = os.path.join(race_path, id)\n",
    "\n",
    "        # Create a subdirectory for the identity if it doesn't exist\n",
    "        if not os.path.exists(identity_save_path):\n",
    "            os.makedirs(identity_save_path)\n",
    "\n",
    "        destination_path = os.path.join(identity_save_path, image_name)\n",
    "        \n",
    "        # Check if the image already exists\n",
    "        if os.path.exists(destination_path):\n",
    "            continue\n",
    "\n",
    "        # Copy the image without converting\n",
    "        shutil.copy(source_path, destination_path)\n",
    "\n",
    "    print('Finished copying images to folders in JPG format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48000/48000 [00:03<00:00, 14421.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished copying training images to folders in JPG format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12000/12000 [00:00<00:00, 14139.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished copying images to folders in JPG format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#just saving one race for now\n",
    "data = pd.read_csv(\"./balanced_data_split.csv\")\n",
    "african_balanced_split = data[data['race']==3]\n",
    "caucasian_balanced_split = data[data['race']==0]\n",
    "\n",
    "african_and_cau = pd.concat([african_balanced_split, caucasian_balanced_split])\n",
    "\n",
    "\n",
    "data_folder_path = \"./race_per_7000\"\n",
    "save_path = \"./\"\n",
    "\n",
    "\n",
    "race_to_label = {'Caucasian': 0, 'Indian': 1, 'Asian': 2, 'African': 3}\n",
    "label_to_race = {0: 'Caucasian', 1: 'Indian', 2: 'Asian', 3: 'African'}\n",
    "\n",
    "save_train_image(african_and_cau, data_folder_path, save_path+'/train', label_to_race)\n",
    "save_test_image(african_and_cau, data_folder_path, save_path+'/test', label_to_race)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#just getting mvp mini data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "african_unbalanced = pd.read_csv(\"./unbalanced_African.csv\")\n",
    "\n",
    "#take first 50 id from african and first 10 caucasian \n",
    "african_only = african_unbalanced[african_unbalanced['race']==3]\n",
    "cau_only = african_unbalanced[african_unbalanced['race']==0]\n",
    "\n",
    "data = pd.concat([african_only, cau_only])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   0%|          | 0/87600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 81520/87600 [01:46<00:07, 768.19it/s] \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './race_per_7000/Caucasian/m.03j2zfm/6-FaceId-0_align.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m race_to_label \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mCaucasian\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mIndian\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAsian\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAfrican\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m3\u001b[39m}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m label_to_race \u001b[39m=\u001b[39m {\u001b[39m0\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mCaucasian\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mIndian\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m2\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mAsian\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m3\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mAfrican\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb#X56sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m save_train_image(data, data_folder_path, save_path\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/con_train_mini\u001b[39;49m\u001b[39m'\u001b[39;49m, label_to_race)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb#X56sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# save_test_image(data, data_folder_path, save_path+'/test', label_to_race)\u001b[39;00m\n",
      "\u001b[1;32m/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb#X56sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb#X56sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# Copy the image without converting\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb#X56sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     shutil\u001b[39m.\u001b[39;49mcopy(source_path, destination_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nina/Desktop/SupContrast-ORE/data/get_data.ipynb#X56sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFinished copying training images to folders in JPG format\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.18/Frameworks/Python.framework/Versions/3.9/lib/python3.9/shutil.py:427\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(dst):\n\u001b[1;32m    426\u001b[0m     dst \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dst, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(src))\n\u001b[0;32m--> 427\u001b[0m copyfile(src, dst, follow_symlinks\u001b[39m=\u001b[39;49mfollow_symlinks)\n\u001b[1;32m    428\u001b[0m copymode(src, dst, follow_symlinks\u001b[39m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    429\u001b[0m \u001b[39mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.18/Frameworks/Python.framework/Versions/3.9/lib/python3.9/shutil.py:264\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    262\u001b[0m     os\u001b[39m.\u001b[39msymlink(os\u001b[39m.\u001b[39mreadlink(src), dst)\n\u001b[1;32m    263\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(src, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fsrc:\n\u001b[1;32m    265\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m             \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(dst, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fdst:\n\u001b[1;32m    267\u001b[0m                 \u001b[39m# macOS\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './race_per_7000/Caucasian/m.03j2zfm/6-FaceId-0_align.jpg'"
     ]
    }
   ],
   "source": [
    "#get dataset\n",
    "#just saving one race for now\n",
    "\n",
    "data_folder_path = \"./race_per_7000\"\n",
    "save_path = \"./\"\n",
    "\n",
    "\n",
    "race_to_label = {'Caucasian': 0, 'Indian': 1, 'Asian': 2, 'African': 3}\n",
    "label_to_race = {0: 'Caucasian', 1: 'Indian', 2: 'Asian', 3: 'African'}\n",
    "\n",
    "save_train_image(data, data_folder_path, save_path+'/con_train_mini', label_to_race)\n",
    "# save_test_image(data, data_folder_path, save_path+'/test', label_to_race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = \"./race_per_7000\"\n",
    "save_path = \"./\"\n",
    "\n",
    "\n",
    "race_to_label = {'Caucasian': 0, 'Indian': 1, 'Asian': 2, 'African': 3}\n",
    "label_to_race = {0: 'Caucasian', 1: 'Indian', 2: 'Asian', 3: 'African'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_prob_dataset_split\n",
    "\n",
    "# from the linear_prob_dataset_split.csv, i want to actually get those images from the race_per_700/ folder and put them into a folder in my current directpry called linear_prob_data, and this folder should have the sub folders for test and train data from the linear_prob_dataset_split.csv\n",
    "# race_per_700 has 4 sub folders for each race. Each race has subfolder for their ids. And each id has their images\n",
    "# race_per_700/\n",
    "    # -> [RACE]\n",
    "    #     -> id_1\n",
    "    #     -> id_2\n",
    "\n",
    "# we should copy the data from race_per_700 that belongs to linear_prob_dataset_split and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_per_7000/Caucasian \n",
    "race_per_700/Caucasian/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images copied and organized into 'linear_prob_data' directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'linear_prob_dataset_split' dataset\n",
    "linear_prob_dataset_split = pd.read_csv(\"linear_prob_dataset_split.csv\")\n",
    "\n",
    "# Define source and destination directories\n",
    "source_dir = \"race_per_7000\"  # Replace with your actual source directory\n",
    "destination_dir = \"linear_prob_data\"\n",
    "race_to_label = {'Caucasian': 0, 'Indian': 1, 'Asian': 2, 'African': 3}\n",
    "label_to_race = {0: 'Caucasian', 1: 'Indian', 2: 'Asian', 3: 'African'}\n",
    "# Create destination directories for test and train if they don't exist\n",
    "for split in [\"test\", \"train\"]:\n",
    "    os.makedirs(os.path.join(destination_dir, split), exist_ok=True)\n",
    "\n",
    "# Iterate through each row in the dataset\n",
    "for _, row in linear_prob_dataset_split.iterrows():\n",
    "    image_name = row['image_name']\n",
    "    race = label_to_race[row['race']]\n",
    "    id = row['id']\n",
    "    split = row['split']\n",
    "\n",
    "    # Define source and destination paths for the image\n",
    "    source_path = os.path.join(source_dir, race, id, image_name)\n",
    "    if split == \"train\":\n",
    "        \n",
    "        destination_path = os.path.join(destination_dir, split, race, id, image_name)\n",
    "        # Create destination directories for each race and ID if they don't exist\n",
    "        os.makedirs(os.path.join(destination_dir, split, race, id), exist_ok=True)\n",
    "\n",
    "    elif split == \"test\":\n",
    "        destination_path = os.path.join(destination_dir, split, id, image_name)\n",
    "        os.makedirs(os.path.join(destination_dir, split, id), exist_ok=True)\n",
    "\n",
    "\n",
    "    # Copy the image to the destination directory\n",
    "    shutil.copy(source_path, destination_path)\n",
    "\n",
    "print(\"Images copied and organized into 'linear_prob_data' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Indian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Excluding race: Asian\n",
      "Images copied and organized into 'linear_prob_data' directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'linear_prob_dataset_split' dataset\n",
    "linear_prob_dataset_split = pd.read_csv(\"linear_prob_dataset_split.csv\")\n",
    "\n",
    "# Define source and destination directories\n",
    "source_dir = \"race_per_7000\"  # Replace with your actual source directory\n",
    "destination_dir = \"linear_prob_data\"\n",
    "label_to_race = {0: 'Caucasian', 1: 'Indian', 2: 'Asian', 3: 'African'}\n",
    "\n",
    "# Create destination directories for test and train if they don't exist\n",
    "for split in [\"test\", \"train\"]:\n",
    "    os.makedirs(os.path.join(destination_dir, split), exist_ok=True)\n",
    "\n",
    "# Define races to exclude from 'test_2'\n",
    "excluded_races = ['Asian', 'Indian']\n",
    "\n",
    "# Iterate through each row in the dataset\n",
    "for _, row in linear_prob_dataset_split.iterrows():\n",
    "    image_name = row['image_name']\n",
    "    race = label_to_race[row['race']]\n",
    "    id = row['id']\n",
    "    split = row['split']\n",
    "\n",
    "    # Check if the race should be excluded from 'test_2'\n",
    "    if split == \"test\" and race in excluded_races:\n",
    "        print(f\"Excluding race: {race}\")\n",
    "        continue\n",
    "\n",
    "    # Define source and destination paths for the image\n",
    "    source_path = os.path.join(source_dir, race, id, image_name)\n",
    "\n",
    "    if split == \"train\":\n",
    "        destination_path = os.path.join(destination_dir, split, race, id, image_name)\n",
    "        # Create destination directories for each race and ID if they don't exist\n",
    "        os.makedirs(os.path.join(destination_dir, split, race, id), exist_ok=True)\n",
    "\n",
    "    elif split == \"test\":\n",
    "        destination_path = os.path.join(destination_dir, split, id, image_name)\n",
    "        os.makedirs(os.path.join(destination_dir, split, id), exist_ok=True)\n",
    "\n",
    "    # Copy the image to the destination directory\n",
    "    shutil.copy(source_path, destination_path)\n",
    "\n",
    "print(\"Images copied and organized into 'linear_prob_data' directory.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
